{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "reading train images...\n",
      "preprocessing train volumes...\n",
      "cropping train volumes...\n",
      "padding train volumes...\n",
      "resizing train volumes...\n",
      "normalizing train volumes...\n",
      "done creating train dataset\n",
      "reading validation images...\n",
      "preprocessing validation volumes...\n",
      "cropping validation volumes...\n",
      "padding validation volumes...\n",
      "resizing validation volumes...\n",
      "normalizing validation volumes...\n",
      "done creating validation dataset\n",
      "Training started!\n",
      "epoch 1 | loss: 0.8778581456704573\n",
      "epoch 1 | val_loss: 0.94542738369533\n",
      "epoch 1 | val_dsc: 0.06358021487835958\n",
      "epoch 2 | loss: 0.8582093498923562\n",
      "epoch 2 | val_loss: 0.9419889875820705\n",
      "epoch 2 | val_dsc: 0.0993890085153238\n",
      "epoch 3 | loss: 0.8521839651194486\n",
      "epoch 3 | val_loss: 0.9403705511774335\n",
      "epoch 3 | val_dsc: 0.30576201897017624\n",
      "epoch 4 | loss: 0.8481140452803988\n",
      "epoch 4 | val_loss: 0.9385898453848702\n",
      "epoch 4 | val_dsc: 0.5177218424346709\n",
      "epoch 5 | loss: 0.8440057535966238\n",
      "epoch 5 | val_loss: 0.9369755983352661\n",
      "epoch 5 | val_dsc: 0.4366385408714625\n",
      "epoch 6 | loss: 0.8399456667177605\n",
      "epoch 6 | val_loss: 0.9347655773162842\n",
      "epoch 6 | val_dsc: 0.6371774439326768\n",
      "epoch 7 | loss: 0.8352306151028835\n",
      "epoch 7 | val_loss: 0.9328950047492981\n",
      "epoch 7 | val_dsc: 0.6411848671690625\n",
      "epoch 8 | loss: 0.8306291428479281\n",
      "epoch 8 | val_loss: 0.9307066457612174\n",
      "epoch 8 | val_dsc: 0.7409437061300067\n",
      "epoch 9 | loss: 0.8255807197455204\n",
      "epoch 9 | val_loss: 0.9284029432705471\n",
      "epoch 9 | val_dsc: 0.7391102148711703\n",
      "epoch 10 | loss: 0.8205655777093136\n",
      "epoch 10 | val_loss: 0.9273615905216762\n",
      "epoch 10 | val_dsc: 0.3896056379297354\n",
      "epoch 11 | loss: 0.8154592739813256\n",
      "epoch 11 | val_loss: 0.9227211986269269\n",
      "epoch 11 | val_dsc: 0.7218292054867802\n",
      "epoch 12 | loss: 0.8084234622391787\n",
      "epoch 12 | val_loss: 0.9197314807346889\n",
      "epoch 12 | val_dsc: 0.6969767621475793\n",
      "epoch 13 | loss: 0.801580806573232\n",
      "epoch 13 | val_loss: 0.9169198615210397\n",
      "epoch 13 | val_dsc: 0.7558346049058418\n",
      "epoch 14 | loss: 0.7941444922577251\n",
      "epoch 14 | val_loss: 0.9124041540282113\n",
      "epoch 14 | val_dsc: 0.6967638098892537\n",
      "epoch 15 | loss: 0.7856408529209368\n",
      "epoch 15 | val_loss: 0.9067536507334027\n",
      "epoch 15 | val_dsc: 0.7609599590265038\n",
      "epoch 16 | loss: 0.7762623239647258\n",
      "epoch 16 | val_loss: 0.902296883719308\n",
      "epoch 16 | val_dsc: 0.7795731329420014\n",
      "epoch 17 | loss: 0.7668945942864274\n",
      "epoch 17 | val_loss: 0.898255433355059\n",
      "epoch 17 | val_dsc: 0.6586335746236216\n",
      "epoch 18 | loss: 0.7563455800215403\n",
      "epoch 18 | val_loss: 0.8923925927707127\n",
      "epoch 18 | val_dsc: 0.7458687571212523\n",
      "epoch 19 | loss: 0.7431985239187876\n",
      "epoch 19 | val_loss: 0.8903850657599313\n",
      "epoch 19 | val_dsc: 0.7264463133770744\n",
      "epoch 20 | loss: 0.7248727715376652\n",
      "epoch 20 | val_loss: 0.8833842703274318\n",
      "epoch 20 | val_dsc: 0.7127228801849833\n",
      "epoch 21 | loss: 0.708456631862756\n",
      "epoch 21 | val_loss: 0.8731367247445243\n",
      "epoch 21 | val_dsc: 0.8036764127212953\n",
      "epoch 22 | loss: 0.6917989100470687\n",
      "epoch 22 | val_loss: 0.8652323314121791\n",
      "epoch 22 | val_dsc: 0.757696591415033\n",
      "epoch 23 | loss: 0.6768350221894004\n",
      "epoch 23 | val_loss: 0.8552505970001221\n",
      "epoch 23 | val_dsc: 0.7670351402768694\n",
      "epoch 24 | loss: 0.6568331745537844\n",
      "epoch 24 | val_loss: 0.8445915579795837\n",
      "epoch 24 | val_dsc: 0.8058490253533623\n",
      "epoch 25 | loss: 0.6363106144197059\n",
      "epoch 25 | val_loss: 0.8342629671096802\n",
      "epoch 25 | val_dsc: 0.8108597084359003\n",
      "epoch 26 | loss: 0.6147208719542532\n",
      "epoch 26 | val_loss: 0.8215733766555786\n",
      "epoch 26 | val_dsc: 0.8454171218398617\n",
      "epoch 27 | loss: 0.5927808257666501\n",
      "epoch 27 | val_loss: 0.8077606473650251\n",
      "epoch 27 | val_dsc: 0.8312919211615029\n",
      "epoch 28 | loss: 0.5702169781381433\n",
      "epoch 28 | val_loss: 0.7943200724465507\n",
      "epoch 28 | val_dsc: 0.8371624360157188\n",
      "epoch 29 | loss: 0.547228860132622\n",
      "epoch 29 | val_loss: 0.7811638968331474\n",
      "epoch 29 | val_dsc: 0.8416294584834206\n",
      "epoch 30 | loss: 0.5238068925611901\n",
      "epoch 30 | val_loss: 0.767198017665318\n",
      "epoch 30 | val_dsc: 0.8342930745454187\n",
      "epoch 31 | loss: 0.5008202131950494\n",
      "epoch 31 | val_loss: 0.7532135248184204\n",
      "epoch 31 | val_dsc: 0.8388416049051169\n",
      "epoch 32 | loss: 0.4777819008538217\n",
      "epoch 32 | val_loss: 0.7383599877357483\n",
      "epoch 32 | val_dsc: 0.8596849900248269\n",
      "epoch 33 | loss: 0.4549076168826132\n",
      "epoch 33 | val_loss: 0.7219962307385036\n",
      "epoch 33 | val_dsc: 0.854898358249281\n",
      "epoch 34 | loss: 0.4329827915538441\n",
      "epoch 34 | val_loss: 0.7038487110819135\n",
      "epoch 34 | val_dsc: 0.8426361623036467\n",
      "epoch 35 | loss: 0.41707275130532007\n",
      "epoch 35 | val_loss: 0.6826033847672599\n",
      "epoch 35 | val_dsc: 0.8137770634654051\n",
      "epoch 36 | loss: 0.39772996396729443\n",
      "epoch 36 | val_loss: 0.6645608714648655\n",
      "epoch 36 | val_dsc: 0.8482988353433836\n",
      "epoch 37 | loss: 0.37105870156577136\n",
      "epoch 37 | val_loss: 0.6471491456031799\n",
      "epoch 37 | val_dsc: 0.8441012412289659\n",
      "epoch 38 | loss: 0.3483125558405211\n",
      "epoch 38 | val_loss: 0.6288926175662449\n",
      "epoch 38 | val_dsc: 0.8595976270274353\n",
      "epoch 39 | loss: 0.328193283442295\n",
      "epoch 39 | val_loss: 0.6143417698996407\n",
      "epoch 39 | val_dsc: 0.8639493860243375\n",
      "epoch 40 | loss: 0.3101163786469084\n",
      "epoch 40 | val_loss: 0.5968075309480939\n",
      "epoch 40 | val_dsc: 0.8656478255428178\n",
      "epoch 41 | loss: 0.2941798387151776\n",
      "epoch 41 | val_loss: 0.5752410037176949\n",
      "epoch 41 | val_dsc: 0.8619435343440406\n",
      "epoch 42 | loss: 0.278652877518625\n",
      "epoch 42 | val_loss: 0.5580163683210101\n",
      "epoch 42 | val_dsc: 0.840774764286363\n",
      "epoch 43 | loss: 0.2616424018686468\n",
      "epoch 43 | val_loss: 0.5417750818388802\n",
      "epoch 43 | val_dsc: 0.8473878707855762\n",
      "epoch 44 | loss: 0.2454472572514505\n",
      "epoch 44 | val_loss: 0.5260613645826068\n",
      "epoch 44 | val_dsc: 0.855931033686927\n",
      "epoch 45 | loss: 0.2315697128122503\n",
      "epoch 45 | val_loss: 0.5135985953467233\n",
      "epoch 45 | val_dsc: 0.8581727000528592\n",
      "epoch 46 | loss: 0.21970674124631015\n",
      "epoch 46 | val_loss: 0.5030210358755929\n",
      "epoch 46 | val_dsc: 0.8499778586304124\n",
      "epoch 47 | loss: 0.20921092322378448\n",
      "epoch 47 | val_loss: 0.4783544625554766\n",
      "epoch 47 | val_dsc: 0.8520384621089387\n",
      "epoch 48 | loss: 0.1984748768083977\n",
      "epoch 48 | val_loss: 0.46422743797302246\n",
      "epoch 48 | val_dsc: 0.8666624159302746\n",
      "epoch 49 | loss: 0.1878783693819335\n",
      "epoch 49 | val_loss: 0.45718434878758024\n",
      "epoch 49 | val_dsc: 0.8617688508473563\n",
      "epoch 50 | loss: 0.17801864038814197\n",
      "epoch 50 | val_loss: 0.4495252115385873\n",
      "epoch 50 | val_dsc: 0.8495674402037163\n",
      "\n",
      "Best validation mean DSC: 0.866662\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-d5e042660d60>:564: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  return np.fromstring(s, np.uint8).reshape((height, width, 4))\n",
      "<ipython-input-12-d5e042660d60>:700: UserWarning: ./TCGA_DU_7014_19860618-57.png is a low contrast image\n",
      "  imsave(filepath, image)\n",
      "<ipython-input-12-d5e042660d60>:700: UserWarning: ./TCGA_DU_6408_19860521-51.png is a low contrast image\n",
      "  imsave(filepath, image)\n",
      "<ipython-input-12-d5e042660d60>:700: UserWarning: ./TCGA_DU_6408_19860521-52.png is a low contrast image\n",
      "  imsave(filepath, image)\n",
      "<ipython-input-12-d5e042660d60>:700: UserWarning: ./TCGA_DU_6408_19860521-53.png is a low contrast image\n",
      "  imsave(filepath, image)\n",
      "<ipython-input-12-d5e042660d60>:700: UserWarning: ./TCGA_DU_6404_19850629-50.png is a low contrast image\n",
      "  imsave(filepath, image)\n",
      "<ipython-input-12-d5e042660d60>:700: UserWarning: ./TCGA_DU_5851_19950428-34.png is a low contrast image\n",
      "  imsave(filepath, image)\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "from tqdm import tqdm\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize, rescale, rotate\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "\n",
    "def crop_sample(x):\n",
    "    volume, mask = x\n",
    "    volume[volume < np.max(volume) * 0.1] = 0\n",
    "    z_projection = np.max(np.max(np.max(volume, axis=-1), axis=-1), axis=-1)\n",
    "    z_nonzero = np.nonzero(z_projection)\n",
    "    z_min = np.min(z_nonzero)\n",
    "    z_max = np.max(z_nonzero) + 1\n",
    "    y_projection = np.max(np.max(np.max(volume, axis=0), axis=-1), axis=-1)\n",
    "    y_nonzero = np.nonzero(y_projection)\n",
    "    y_min = np.min(y_nonzero)\n",
    "    y_max = np.max(y_nonzero) + 1\n",
    "    x_projection = np.max(np.max(np.max(volume, axis=0), axis=0), axis=-1)\n",
    "    x_nonzero = np.nonzero(x_projection)\n",
    "    x_min = np.min(x_nonzero)\n",
    "    x_max = np.max(x_nonzero) + 1\n",
    "    return (\n",
    "        volume[z_min:z_max, y_min:y_max, x_min:x_max],\n",
    "        mask[z_min:z_max, y_min:y_max, x_min:x_max],\n",
    "    )\n",
    "\n",
    "\n",
    "def pad_sample(x):\n",
    "    volume, mask = x\n",
    "    a = volume.shape[1]\n",
    "    b = volume.shape[2]\n",
    "    if a == b:\n",
    "        return volume, mask\n",
    "    diff = (max(a, b) - min(a, b)) / 2.0\n",
    "    if a > b:\n",
    "        padding = ((0, 0), (0, 0), (int(np.floor(diff)), int(np.ceil(diff))))\n",
    "    else:\n",
    "        padding = ((0, 0), (int(np.floor(diff)), int(np.ceil(diff))), (0, 0))\n",
    "    mask = np.pad(mask, padding, mode=\"constant\", constant_values=0)\n",
    "    padding = padding + ((0, 0),)\n",
    "    volume = np.pad(volume, padding, mode=\"constant\", constant_values=0)\n",
    "    return volume, mask\n",
    "\n",
    "\n",
    "def resize_sample(x, size=256):\n",
    "    volume, mask = x\n",
    "    v_shape = volume.shape\n",
    "    out_shape = (v_shape[0], size, size)\n",
    "    mask = resize(\n",
    "        mask,\n",
    "        output_shape=out_shape,\n",
    "        order=0,\n",
    "        mode=\"constant\",\n",
    "        cval=0,\n",
    "        anti_aliasing=False,\n",
    "    )\n",
    "    out_shape = out_shape + (v_shape[3],)\n",
    "    volume = resize(\n",
    "        volume,\n",
    "        output_shape=out_shape,\n",
    "        order=2,\n",
    "        mode=\"constant\",\n",
    "        cval=0,\n",
    "        anti_aliasing=False,\n",
    "    )\n",
    "    return volume, mask\n",
    "\n",
    "\n",
    "def normalize_volume(volume):\n",
    "    p10 = np.percentile(volume, 10)\n",
    "    p99 = np.percentile(volume, 99)\n",
    "    volume = rescale_intensity(volume, in_range=(p10, p99))\n",
    "    m = np.mean(volume, axis=(0, 1, 2))\n",
    "    s = np.std(volume, axis=(0, 1, 2))\n",
    "    volume = (volume - m) / s\n",
    "    return volume\n",
    "\n",
    "class BrainSegmentationDataset(Dataset):\n",
    "    \"\"\"Brain MRI dataset for FLAIR abnormality segmentation\"\"\"\n",
    "\n",
    "    in_channels = 3\n",
    "    out_channels = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir,\n",
    "        transform=None,\n",
    "        image_size=256,\n",
    "        subset=\"train\",\n",
    "        random_sampling=True,\n",
    "        seed=42,\n",
    "    ):\n",
    "        assert subset in [\"all\", \"train\", \"validation\"]\n",
    "\n",
    "        # read images\n",
    "        volumes = {}\n",
    "        masks = {}\n",
    "        print(\"reading {} images...\".format(subset))\n",
    "        for (dirpath, dirnames, filenames) in os.walk(images_dir):\n",
    "            image_slices = []\n",
    "            mask_slices = []\n",
    "            for filename in sorted(\n",
    "                filter(lambda f: \".tif\" in f, filenames),\n",
    "                key=lambda x: int(x.split(\".\")[-2].split(\"_\")[4]),\n",
    "            ):\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                if \"mask\" in filename:\n",
    "                    mask_slices.append(imread(filepath, as_gray=True))\n",
    "                else:\n",
    "                    image_slices.append(imread(filepath))\n",
    "            if len(image_slices) > 0:\n",
    "                patient_id = dirpath.split(\"/\")[-1]\n",
    "                volumes[patient_id] = np.array(image_slices[1:-1])\n",
    "                masks[patient_id] = np.array(mask_slices[1:-1])\n",
    "\n",
    "        self.patients = sorted(volumes)\n",
    "\n",
    "        # select cases to subset\n",
    "        if not subset == \"all\":\n",
    "            random.seed(seed)\n",
    "            validation_patients = random.sample(self.patients, k=10)\n",
    "            if subset == \"validation\":\n",
    "                self.patients = validation_patients\n",
    "            else:\n",
    "                self.patients = sorted(\n",
    "                    list(set(self.patients).difference(validation_patients))\n",
    "                )\n",
    "\n",
    "        print(\"preprocessing {} volumes...\".format(subset))\n",
    "        # create list of tuples (volume, mask)\n",
    "        self.volumes = [(volumes[k], masks[k]) for k in self.patients]\n",
    "\n",
    "        print(\"cropping {} volumes...\".format(subset))\n",
    "        # crop to smallest enclosing volume\n",
    "        self.volumes = [crop_sample(v) for v in self.volumes]\n",
    "\n",
    "        print(\"padding {} volumes...\".format(subset))\n",
    "        # pad to square\n",
    "        self.volumes = [pad_sample(v) for v in self.volumes]\n",
    "\n",
    "        print(\"resizing {} volumes...\".format(subset))\n",
    "        # resize\n",
    "        self.volumes = [resize_sample(v, size=image_size) for v in self.volumes]\n",
    "\n",
    "        print(\"normalizing {} volumes...\".format(subset))\n",
    "        # normalize channel-wise\n",
    "        self.volumes = [(normalize_volume(v), m) for v, m in self.volumes]\n",
    "\n",
    "        # probabilities for sampling slices based on masks\n",
    "        self.slice_weights = [m.sum(axis=-1).sum(axis=-1) for v, m in self.volumes]\n",
    "        self.slice_weights = [\n",
    "            (s + (s.sum() * 0.1 / len(s))) / (s.sum() * 1.1) for s in self.slice_weights\n",
    "        ]\n",
    "\n",
    "        # add channel dimension to masks\n",
    "        self.volumes = [(v, m[..., np.newaxis]) for (v, m) in self.volumes]\n",
    "\n",
    "        print(\"done creating {} dataset\".format(subset))\n",
    "\n",
    "        # create global index for patient and slice (idx -> (p_idx, s_idx))\n",
    "        num_slices = [v.shape[0] for v, m in self.volumes]\n",
    "        self.patient_slice_index = list(\n",
    "            zip(\n",
    "                sum([[i] * num_slices[i] for i in range(len(num_slices))], []),\n",
    "                sum([list(range(x)) for x in num_slices], []),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.random_sampling = random_sampling\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_slice_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient = self.patient_slice_index[idx][0]\n",
    "        slice_n = self.patient_slice_index[idx][1]\n",
    "\n",
    "        if self.random_sampling:\n",
    "            patient = np.random.randint(len(self.volumes))\n",
    "            slice_n = np.random.choice(\n",
    "                range(self.volumes[patient][0].shape[0]), p=self.slice_weights[patient]\n",
    "            )\n",
    "\n",
    "        v, m = self.volumes[patient]\n",
    "        image = v[slice_n]\n",
    "        mask = m[slice_n]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image, mask = self.transform((image, mask))\n",
    "\n",
    "        # fix dimensions (C, H, W)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        mask = mask.transpose(2, 0, 1)\n",
    "\n",
    "        image_tensor = torch.from_numpy(image.astype(np.float32))\n",
    "        mask_tensor = torch.from_numpy(mask.astype(np.float32))\n",
    "\n",
    "        # return tensors\n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "def transforms(scale=None, angle=None, flip_prob=None):\n",
    "    transform_list = []\n",
    "\n",
    "    if scale is not None:\n",
    "        transform_list.append(Scale(scale))\n",
    "    if angle is not None:\n",
    "        transform_list.append(Rotate(angle))\n",
    "    if flip_prob is not None:\n",
    "        transform_list.append(HorizontalFlip(flip_prob))\n",
    "\n",
    "    return Compose(transform_list)\n",
    "\n",
    "\n",
    "class Scale(object):\n",
    "\n",
    "    def __init__(self, scale):\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample\n",
    "\n",
    "        img_size = image.shape[0]\n",
    "\n",
    "        scale = np.random.uniform(low=1.0 - self.scale, high=1.0 + self.scale)\n",
    "\n",
    "        image = rescale(\n",
    "            image,\n",
    "            (scale, scale),\n",
    "            multichannel=True,\n",
    "            preserve_range=True,\n",
    "            mode=\"constant\",\n",
    "            anti_aliasing=False,\n",
    "        )\n",
    "        mask = rescale(\n",
    "            mask,\n",
    "            (scale, scale),\n",
    "            order=0,\n",
    "            multichannel=True,\n",
    "            preserve_range=True,\n",
    "            mode=\"constant\",\n",
    "            anti_aliasing=False,\n",
    "        )\n",
    "\n",
    "        if scale < 1.0:\n",
    "            diff = (img_size - image.shape[0]) / 2.0\n",
    "            padding = ((int(np.floor(diff)), int(np.ceil(diff))),) * 2 + ((0, 0),)\n",
    "            image = np.pad(image, padding, mode=\"constant\", constant_values=0)\n",
    "            mask = np.pad(mask, padding, mode=\"constant\", constant_values=0)\n",
    "        else:\n",
    "            x_min = (image.shape[0] - img_size) // 2\n",
    "            x_max = x_min + img_size\n",
    "            image = image[x_min:x_max, x_min:x_max, ...]\n",
    "            mask = mask[x_min:x_max, x_min:x_max, ...]\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class Rotate(object):\n",
    "\n",
    "    def __init__(self, angle):\n",
    "        self.angle = angle\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample\n",
    "\n",
    "        angle = np.random.uniform(low=-self.angle, high=self.angle)\n",
    "        image = rotate(image, angle, resize=False, preserve_range=True, mode=\"constant\")\n",
    "        mask = rotate(\n",
    "            mask, angle, resize=False, order=0, preserve_range=True, mode=\"constant\"\n",
    "        )\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class HorizontalFlip(object):\n",
    "\n",
    "    def __init__(self, flip_prob):\n",
    "        self.flip_prob = flip_prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample\n",
    "\n",
    "        if np.random.rand() > self.flip_prob:\n",
    "            return image, mask\n",
    "\n",
    "        image = np.fliplr(image).copy()\n",
    "        mask = np.fliplr(mask).copy()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        return torch.sigmoid(self.conv(dec1))\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        name + \"conv1\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=in_channels,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    (\n",
    "                        name + \"conv2\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=features,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = 1.0\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.size() == y_true.size()\n",
    "        y_pred = y_pred[:, 0].contiguous().view(-1)\n",
    "        y_true = y_true[:, 0].contiguous().view(-1)\n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        dsc = (2. * intersection + self.smooth) / (\n",
    "            y_pred.sum() + y_true.sum() + self.smooth\n",
    "        )\n",
    "        return 1. - dsc\n",
    "\n",
    "\n",
    "def log_images(x, y_true, y_pred, channel=1):\n",
    "    images = []\n",
    "    x_np = x[:, channel].cpu().numpy()\n",
    "    y_true_np = y_true[:, 0].cpu().numpy()\n",
    "    y_pred_np = y_pred[:, 0].cpu().numpy()\n",
    "    for i in range(x_np.shape[0]):\n",
    "        image = gray2rgb(np.squeeze(x_np[i]))\n",
    "        image = outline(image, y_pred_np[i], color=[255, 0, 0])\n",
    "        image = outline(image, y_true_np[i], color=[0, 255, 0])\n",
    "        images.append(image)\n",
    "    return images\n",
    "\n",
    "\n",
    "def gray2rgb(image):\n",
    "    w, h = image.shape\n",
    "    image += np.abs(np.min(image))\n",
    "    image_max = np.abs(np.max(image))\n",
    "    if image_max > 0:\n",
    "        image /= image_max\n",
    "    ret = np.empty((w, h, 3), dtype=np.uint8)\n",
    "    ret[:, :, 2] = ret[:, :, 1] = ret[:, :, 0] = image * 255\n",
    "    return ret\n",
    "\n",
    "\n",
    "def outline(image, mask, color):\n",
    "    mask = np.round(mask)\n",
    "    yy, xx = np.nonzero(mask)\n",
    "    for y, x in zip(yy, xx):\n",
    "        if 0.0 < np.mean(mask[max(0, y - 1) : y + 2, max(0, x - 1) : x + 2]) < 1.0:\n",
    "            image[max(0, y) : y + 1, max(0, x) : x + 1] = color\n",
    "    return image\n",
    "\n",
    "\n",
    "def data_loaders(batch_size, workers, image_size, aug_scale, aug_angle):\n",
    "    dataset_train, dataset_valid = datasets(\"/home/tudor/PySyft/Q&AId/core/kaggle_3m\", image_size, aug_scale, aug_angle)\n",
    "\n",
    "    def worker_init(worker_id):\n",
    "        np.random.seed(42 + worker_id)\n",
    "\n",
    "    loader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=workers,\n",
    "        worker_init_fn=worker_init,\n",
    "    )\n",
    "    loader_valid = DataLoader(\n",
    "        dataset_valid,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=False,\n",
    "        num_workers=workers,\n",
    "        worker_init_fn=worker_init,\n",
    "    )\n",
    "\n",
    "    return loader_train, loader_valid\n",
    "\n",
    "\n",
    "def datasets(images, image_size, aug_scale, aug_angle):\n",
    "    train = BrainSegmentationDataset(\n",
    "        images_dir=images,\n",
    "        subset=\"train\",\n",
    "        image_size=image_size,\n",
    "        transform=transforms(scale=aug_scale, angle=aug_angle, flip_prob=0.5),\n",
    "    )\n",
    "    valid = BrainSegmentationDataset(\n",
    "        images_dir=images,\n",
    "        subset=\"validation\",\n",
    "        image_size=image_size,\n",
    "        random_sampling=False,\n",
    "    )\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "def dsc(y_pred, y_true):\n",
    "    y_pred = np.round(y_pred).astype(int)\n",
    "    y_true = np.round(y_true).astype(int)\n",
    "    return np.sum(y_pred[y_true == 1]) * 2.0 / (np.sum(y_pred) + np.sum(y_true))\n",
    "\n",
    "\n",
    "def dsc_distribution(volumes):\n",
    "    dsc_dict = {}\n",
    "    for p in volumes:\n",
    "        y_pred = volumes[p][1]\n",
    "        y_true = volumes[p][2]\n",
    "        dsc_dict[p] = dsc(y_pred, y_true)\n",
    "    return dsc_dict\n",
    "\n",
    "\n",
    "def dsc_per_volume(validation_pred, validation_true, patient_slice_index):\n",
    "    dsc_list = []\n",
    "    num_slices = np.bincount([p[0] for p in patient_slice_index])\n",
    "    index = 0\n",
    "    for p in range(len(num_slices)):\n",
    "        y_pred = np.array(validation_pred[index : index + num_slices[p]])\n",
    "        y_true = np.array(validation_true[index : index + num_slices[p]])\n",
    "        dsc_list.append(dsc(y_pred, y_true))\n",
    "        index += num_slices[p]\n",
    "    return dsc_list\n",
    "\n",
    "\n",
    "def postprocess_per_volume(\n",
    "    input_list, pred_list, true_list, patient_slice_index, patients\n",
    "):\n",
    "    volumes = {}\n",
    "    num_slices = np.bincount([p[0] for p in patient_slice_index])\n",
    "    index = 0\n",
    "    for p in range(len(num_slices)):\n",
    "        volume_in = np.array(input_list[index : index + num_slices[p]])\n",
    "        volume_pred = np.round(\n",
    "            np.array(pred_list[index : index + num_slices[p]])\n",
    "        ).astype(int)\n",
    "        volume_true = np.array(true_list[index : index + num_slices[p]])\n",
    "        volumes[patients[p]] = (volume_in, volume_pred, volume_true)\n",
    "        index += num_slices[p]\n",
    "    return volumes\n",
    "\n",
    "\n",
    "def log_loss_summary(loss, step, prefix=\"\"):\n",
    "    print(\"epoch {} | {}: {}\".format(step + 1, prefix + \"loss\", np.mean(loss)))\n",
    "\n",
    "def log_scalar_summary(tag, value, step):\n",
    "    print(\"epoch {} | {}: {}\".format(step + 1, tag, value))\n",
    "\n",
    "\n",
    "def plot_dsc(dsc_dist):\n",
    "    y_positions = np.arange(len(dsc_dist))\n",
    "    dsc_dist = sorted(dsc_dist.items(), key=lambda x: x[1])\n",
    "    values = [x[1] for x in dsc_dist]\n",
    "    labels = [x[0] for x in dsc_dist]\n",
    "    labels = [\"_\".join(l.split(\"_\")[1:-1]) for l in labels]\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    canvas = FigureCanvasAgg(fig)\n",
    "    plt.barh(y_positions, values, align=\"center\", color=\"skyblue\")\n",
    "    plt.yticks(y_positions, labels)\n",
    "    plt.xticks(np.arange(0.0, 1.0, 0.1))\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.gca().axvline(np.mean(values), color=\"tomato\", linewidth=2)\n",
    "    plt.gca().axvline(np.median(values), color=\"forestgreen\", linewidth=2)\n",
    "    plt.xlabel(\"Dice coefficient\", fontsize=\"x-large\")\n",
    "    plt.gca().xaxis.grid(color=\"silver\", alpha=0.5, linestyle=\"--\", linewidth=1)\n",
    "    plt.tight_layout()\n",
    "    canvas.draw()\n",
    "    plt.close()\n",
    "    s, (width, height) = canvas.print_to_buffer()\n",
    "    return np.fromstring(s, np.uint8).reshape((height, width, 4))\n",
    "\n",
    "\n",
    "batch_size = 50\n",
    "epochs = 50\n",
    "lr = 0.0001\n",
    "workers = 2\n",
    "weights = \"./\"\n",
    "image_size = 224\n",
    "aug_scale = 0.05\n",
    "aug_angle = 15\n",
    "\n",
    "\n",
    "def train_validate():\n",
    "    device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda:0\")\n",
    "    print(device)\n",
    "    loader_train, loader_valid = data_loaders(batch_size, workers, image_size, aug_scale, aug_angle)\n",
    "    loaders = {\"train\": loader_train, \"valid\": loader_valid}\n",
    "    \n",
    "    unet = UNet(in_channels=BrainSegmentationDataset.in_channels, out_channels=BrainSegmentationDataset.out_channels)\n",
    "    unet.to(device)\n",
    "    \n",
    "    dsc_loss = DiceLoss()\n",
    "    best_validation_dsc = 0.0\n",
    "    \n",
    "    optimizer = optim.Adam(unet.parameters(), lr=lr)\n",
    "    \n",
    "    loss_train = []\n",
    "    loss_valid = []\n",
    "    \n",
    "    step = 0\n",
    "    print(\"Training started!\")\n",
    "    for epoch in range(epochs):\n",
    "        for phase in [\"train\", \"valid\"]:\n",
    "            if phase == \"train\":\n",
    "                unet.train()\n",
    "            else:\n",
    "                unet.eval()\n",
    "    \n",
    "            validation_pred = []\n",
    "            validation_true = []\n",
    "    \n",
    "            for i, data in enumerate(loaders[phase]):\n",
    "                if phase == \"train\":\n",
    "                    step += 1\n",
    "                \n",
    "                x, y_true = data\n",
    "                x, y_true = x.to(device), y_true.to(device)\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    y_pred = unet(x)\n",
    "    \n",
    "                    loss = dsc_loss(y_pred, y_true)\n",
    "    \n",
    "                    if phase == \"valid\":\n",
    "                        loss_valid.append(loss.item())\n",
    "                        y_pred_np = y_pred.detach().cpu().numpy()\n",
    "                        validation_pred.extend(\n",
    "                            [y_pred_np[s] for s in range(y_pred_np.shape[0])]\n",
    "                        )\n",
    "                        y_true_np = y_true.detach().cpu().numpy()\n",
    "                        validation_true.extend(\n",
    "                            [y_true_np[s] for s in range(y_true_np.shape[0])]\n",
    "                        )\n",
    "                        \n",
    "                    if phase == \"train\":\n",
    "                        loss_train.append(loss.item())\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "    \n",
    "            if phase == \"train\":\n",
    "                log_loss_summary(loss_train, epoch)\n",
    "                loss_train = []\n",
    "\n",
    "            if phase == \"valid\":\n",
    "                log_loss_summary(loss_valid, epoch, prefix=\"val_\")\n",
    "                mean_dsc = np.mean(\n",
    "                    dsc_per_volume(\n",
    "                        validation_pred,\n",
    "                        validation_true,\n",
    "                        loader_valid.dataset.patient_slice_index,\n",
    "                    )\n",
    "                )\n",
    "                log_scalar_summary(\"val_dsc\", mean_dsc, epoch)\n",
    "                if mean_dsc > best_validation_dsc:\n",
    "                    best_validation_dsc = mean_dsc\n",
    "                    torch.save(unet.state_dict(), os.path.join(weights, \"unet.pt\"))\n",
    "                loss_valid = []\n",
    "    \n",
    "    print(\"\\nBest validation mean DSC: {:4f}\\n\".format(best_validation_dsc))\n",
    "    \n",
    " \n",
    "\n",
    "train_validate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [32, 3, 3, 3], but got 3-dimensional input of size [3, 256, 256] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3830c0371309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0my_pred_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpred_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/total_setup/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d5e042660d60>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0menc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0menc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0menc3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/total_setup/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/total_setup/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/total_setup/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/total_setup/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/total_setup/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    347\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 349\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    350\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [32, 3, 3, 3], but got 3-dimensional input of size [3, 256, 256] instead"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "unet = UNet()\n",
    "state_dict = torch.load(os.path.join(weights, \"./unet.pt\"))\n",
    "unet.load_state_dict(state_dict)\n",
    "unet.eval()\n",
    "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda:0\")\n",
    "\n",
    "data = [\"/home/tudor/PySyft/Q&AId/core/kaggle_3m/TCGA_DU_6408_19860521/TCGA_DU_6408_19860521_5.tif\"]\n",
    "input_list = []\n",
    "pred_list = []\n",
    "\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "for i, data in enumerate(data):\n",
    "    img = data_transform(Image.open(data))\n",
    "    x = img\n",
    "    x = x.to(device)\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        y_pred = unet(x)\n",
    "        y_pred_np = y_pred.detach().cpu().numpy()\n",
    "        pred_list.extend([y_pred_np[s] for s in range(y_pred_np.shape[0])])\n",
    "        x_np = x.detach().cpu().numpy()\n",
    "        input_list.extend([x_np[s] for s in range(x_np.shape[0])])\n",
    "\n",
    "volumes = postprocess_per_volume(\n",
    "    input_list,\n",
    "    pred_list,\n",
    "    loader_valid.dataset.patient_slice_index,\n",
    "    loader_valid.dataset.patients,\n",
    ")\n",
    "\n",
    "dsc_dist = dsc_distribution(volumes)\n",
    "\n",
    "dsc_dist_plot = plot_dsc(dsc_dist)\n",
    "imsave(\"./dsc.png\", dsc_dist_plot)\n",
    "\n",
    "for p in volumes:\n",
    "    x = volumes[p][0]\n",
    "    y_pred = volumes[p][1]\n",
    "    for s in range(x.shape[0]):\n",
    "        image = gray2rgb(x[s, 1])  # channel 1 is for FLAIR\n",
    "        image = outline(image, y_pred[s, 0], color=[255, 0, 0])\n",
    "        filename = \"{}-{}.png\".format(p, str(s).zfill(2))\n",
    "        filepath = os.path.join(\"./\", filename)\n",
    "        imsave(filepath, image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainSegmentationDataset(Dataset):\n",
    "    \"\"\"Brain MRI dataset for FLAIR abnormality segmentation\"\"\"\n",
    "\n",
    "    in_channels = 3\n",
    "    out_channels = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir,\n",
    "        transform=None,\n",
    "        image_size=256,\n",
    "        subset=\"train\",\n",
    "        random_sampling=True,\n",
    "        seed=42,\n",
    "    ):\n",
    "        assert subset in [\"all\", \"train\", \"validation\"]\n",
    "\n",
    "        # read images\n",
    "        volumes = {}\n",
    "        masks = {}\n",
    "        print(\"reading {} images...\".format(subset))\n",
    "        for (dirpath, dirnames, filenames) in os.walk(images_dir):\n",
    "            image_slices = []\n",
    "            mask_slices = []\n",
    "            for filename in sorted(\n",
    "                filter(lambda f: \".tif\" in f, filenames),\n",
    "                key=lambda x: int(x.split(\".\")[-2].split(\"_\")[4]),\n",
    "            ):\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                if \"mask\" in filename:\n",
    "                    mask_slices.append(imread(filepath, as_gray=True))\n",
    "                else:\n",
    "                    image_slices.append(imread(filepath))\n",
    "            if len(image_slices) > 0:\n",
    "                patient_id = dirpath.split(\"/\")[-1]\n",
    "                volumes[patient_id] = np.array(image_slices[1:-1])\n",
    "                masks[patient_id] = np.array(mask_slices[1:-1])\n",
    "\n",
    "        self.patients = sorted(volumes)\n",
    "\n",
    "        # select cases to subset\n",
    "        if not subset == \"all\":\n",
    "            random.seed(seed)\n",
    "            validation_patients = random.sample(self.patients, k=10)\n",
    "            if subset == \"validation\":\n",
    "                self.patients = validation_patients\n",
    "            else:\n",
    "                self.patients = sorted(\n",
    "                    list(set(self.patients).difference(validation_patients))\n",
    "                )\n",
    "\n",
    "        print(\"preprocessing {} volumes...\".format(subset))\n",
    "        # create list of tuples (volume, mask)\n",
    "        self.volumes = [(volumes[k], masks[k]) for k in self.patients]\n",
    "\n",
    "        print(\"cropping {} volumes...\".format(subset))\n",
    "        # crop to smallest enclosing volume\n",
    "        self.volumes = [crop_sample(v) for v in self.volumes]\n",
    "\n",
    "        print(\"padding {} volumes...\".format(subset))\n",
    "        # pad to square\n",
    "        self.volumes = [pad_sample(v) for v in self.volumes]\n",
    "\n",
    "        print(\"resizing {} volumes...\".format(subset))\n",
    "        # resize\n",
    "        self.volumes = [resize_sample(v, size=image_size) for v in self.volumes]\n",
    "\n",
    "        print(\"normalizing {} volumes...\".format(subset))\n",
    "        # normalize channel-wise\n",
    "        self.volumes = [(normalize_volume(v), m) for v, m in self.volumes]\n",
    "\n",
    "        # probabilities for sampling slices based on masks\n",
    "        self.slice_weights = [m.sum(axis=-1).sum(axis=-1) for v, m in self.volumes]\n",
    "        self.slice_weights = [\n",
    "            (s + (s.sum() * 0.1 / len(s))) / (s.sum() * 1.1) for s in self.slice_weights\n",
    "        ]\n",
    "\n",
    "        # add channel dimension to masks\n",
    "        self.volumes = [(v, m[..., np.newaxis]) for (v, m) in self.volumes]\n",
    "\n",
    "        print(\"done creating {} dataset\".format(subset))\n",
    "\n",
    "        # create global index for patient and slice (idx -> (p_idx, s_idx))\n",
    "        num_slices = [v.shape[0] for v, m in self.volumes]\n",
    "        self.patient_slice_index = list(\n",
    "            zip(\n",
    "                sum([[i] * num_slices[i] for i in range(len(num_slices))], []),\n",
    "                sum([list(range(x)) for x in num_slices], []),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.random_sampling = random_sampling\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_slice_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient = self.patient_slice_index[idx][0]\n",
    "        slice_n = self.patient_slice_index[idx][1]\n",
    "\n",
    "        if self.random_sampling:\n",
    "            patient = np.random.randint(len(self.volumes))\n",
    "            slice_n = np.random.choice(\n",
    "                range(self.volumes[patient][0].shape[0]), p=self.slice_weights[patient]\n",
    "            )\n",
    "\n",
    "        v, m = self.volumes[patient]\n",
    "        image = v[slice_n]\n",
    "        mask = m[slice_n]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image, mask = self.transform((image, mask))\n",
    "\n",
    "        # fix dimensions (C, H, W)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        mask = mask.transpose(2, 0, 1)\n",
    "\n",
    "        image_tensor = torch.from_numpy(image.astype(np.float32))\n",
    "        mask_tensor = torch.from_numpy(mask.astype(np.float32))\n",
    "\n",
    "        # return tensors\n",
    "        return image_tensor, mask_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
